{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from maglev_env import MagneticEnv, DT\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_points_on_circle(radius, center, z, n):\n",
    "    h, k = center\n",
    "    angles = np.linspace(0, 2*np.pi, n, endpoint=False)  # Generate equally spaced angles\n",
    "    x = h + radius * np.cos(angles)\n",
    "    y = k + radius * np.sin(angles)\n",
    "    z_values = np.full_like(x, z)  # Create an array filled with the z value\n",
    "    points = np.column_stack((x, y, z_values))\n",
    "    return points\n",
    "\n",
    "# Example usage:\n",
    "radius = 2\n",
    "center = (0, 0)\n",
    "num_points = 8\n",
    "\n",
    "circle_points = generate_points_on_circle(radius, center, 0, num_points)\n",
    "mag_coords = list(circle_points)\n",
    "mag_coords.append(np.array([0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    # exp_name: str = os.path.abspath('')[: -len(\".py\")]\n",
    "    exp_name: str = \"LevitationLearn_SAC\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = \"me\"\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"Hopper-v4\"\n",
    "    \"\"\"the environment id of the task\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    buffer_size: int = int(1e6)\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.005\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 256\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    learning_starts: int = 5e3\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_lr: float = 3e-4\n",
    "    \"\"\"the learning rate of the policy network optimizer\"\"\"\n",
    "    q_lr: float = 1e-3\n",
    "    \"\"\"the learning rate of the Q network network optimizer\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.\n",
    "    \"\"\"the frequency of updates for the target nerworks\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "    alpha: float = 0.2\n",
    "    \"\"\"Entropy regularization coefficient.\"\"\"\n",
    "    autotune: bool = True\n",
    "    \"\"\"automatic tuning of the entropy coefficient\"\"\"\n",
    "\n",
    "    #Custom environment arguments:\n",
    "    spawn_range = ((-1.5,1.5),(-1.5,1.5),(2,3))\n",
    "    \"\"\"Spawn range (min, max) of xyz of the ball starting position\"\"\"\n",
    "    desired_range = ((-1.2,1.2),(-1.2,1.2),(2,2.8))\n",
    "    \"\"\"Spawn range (min, max) of xyz of the ball's desired position\"\"\"\n",
    "    mag_coords = mag_coords\n",
    "    \"\"\"XYZ positions of all electromagnets\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def make_mag_env(seed, mag_coords, DT, spawn_range, desired_range):\n",
    "    def thunk():\n",
    "        env = MagneticEnv(mag_coords, DT, spawn_range, desired_range)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low)[0] / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low)[0] / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "if sb3.__version__ < \"2.0\":\n",
    "    raise ValueError(\n",
    "        \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# args = tyro.cli(Args)\n",
    "args = Args()\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "num_envs = 3\n",
    "envs = gym.vector.AsyncVectorEnv([make_mag_env(args.seed, args.mag_coords, DT, args.spawn_range,args.desired_range)] * num_envs)\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "max_action = float(envs.single_action_space.high[0])\n",
    "\n",
    "actor = Actor(envs).to(device)\n",
    "qf1 = SoftQNetwork(envs).to(device)\n",
    "qf2 = SoftQNetwork(envs).to(device)\n",
    "qf1_target = SoftQNetwork(envs).to(device)\n",
    "qf2_target = SoftQNetwork(envs).to(device)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
    "else:\n",
    "    alpha = args.alpha\n",
    "\n",
    "envs.single_observation_space.dtype = np.float32\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    n_envs= num_envs,\n",
    "    handle_timeout_termination=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = None\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed, options=(args.spawn_range,args.desired_range))\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        if start_time is None: start_time = time.time()\n",
    "        actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "        actions = actions.detach().cpu().numpy()\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    # print(rewards)\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos:\n",
    "        for info in infos[\"final_info\"]:\n",
    "            if info is None: continue\n",
    "            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            break\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, trunc in enumerate(truncations):\n",
    "        if trunc:\n",
    "            real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "    # if num_envs == 1:\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "    # else: rb.extend(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)\n",
    "            qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "            qf2_next_target = qf2_target(data.next_observations, next_state_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "        qf2_a_values = qf2(data.observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support\n",
    "            for _ in range(\n",
    "                args.policy_frequency\n",
    "            ):  # compensate for the delay by doing 'actor_update_interval' instead of 1\n",
    "                pi, log_pi, _ = actor.get_action(data.observations)\n",
    "                qf1_pi = qf1(data.observations, pi)\n",
    "                qf2_pi = qf2(data.observations, pi)\n",
    "                min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                if args.autotune:\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi, _ = actor.get_action(data.observations)\n",
    "                    alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "        # update the target networks\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/alpha\", alpha, global_step)\n",
    "            print(\"SPS:\", int((global_step -args.learning_starts) / (time.time() - start_time)))\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "            if args.autotune:\n",
    "                writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "envs.close()\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
