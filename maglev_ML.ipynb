{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import keyboard\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from maglev_env import MagneticEnv, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE think more about these values\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 16\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        # NOTE do we want relu on this?\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         for each normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted means of the action space's normal distribution\n",
    "            action_stddevs: predicted standard deviation of the action space's normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs\n",
    "\n",
    "class Policy:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm.\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        self.action_space_dims = action_space_dims\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-4  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns action(s), conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: Action(s) to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        action_means = action_means.squeeze()\n",
    "        action_stddevs = action_stddevs.squeeze()\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample all actions action\n",
    "        actions = np.zeros(self.action_space_dims)\n",
    "        for action_dim in range(self.action_space_dims):\n",
    "            distrib = Normal(action_means[action_dim] + self.eps, action_stddevs[action_dim] + self.eps)\n",
    "            action = distrib.sample()\n",
    "            prob = distrib.log_prob(action)\n",
    "            actions[action_dim] = action.numpy()\n",
    "\n",
    "            self.probs.append(prob)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   6%|▌         | 6/100 [00:00<00:03, 27.86it/s, Episode Reward:=Epoch: 5 Reward: -28.101566359561172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 0: -25.073585825322265\n",
      "Avg Reward Episode 1: -28.80051930614039\n",
      "Avg Reward Episode 2: -27.630517923777745\n",
      "Avg Reward Episode 3: -29.26097186549937\n",
      "Avg Reward Episode 4: -29.315144118485996\n",
      "Avg Reward Episode 5: -28.101566359561172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   9%|▉         | 9/100 [00:00<00:03, 25.74it/s, Episode Reward:=Epoch: 11 Reward: -28.69666634747203] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 6: -29.98484976532892\n",
      "Avg Reward Episode 7: -28.465264022737383\n",
      "Avg Reward Episode 8: -27.78811007354065\n",
      "Avg Reward Episode 9: -28.60130389492188\n",
      "Avg Reward Episode 10: -29.363820437929633\n",
      "Avg Reward Episode 11: -28.69666634747203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  17%|█▋        | 17/100 [00:00<00:02, 28.74it/s, Episode Reward:=Epoch: 18 Reward: -29.636492781256525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 12: -27.708396934592884\n",
      "Avg Reward Episode 13: -28.360880877294772\n",
      "Avg Reward Episode 14: -28.577541207695855\n",
      "Avg Reward Episode 15: -29.362511377080462\n",
      "Avg Reward Episode 16: -29.1845941280002\n",
      "Avg Reward Episode 17: -29.2245177396441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  23%|██▎       | 23/100 [00:00<00:02, 28.71it/s, Episode Reward:=Epoch: 24 Reward: -29.769743198644182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 18: -29.636492781256525\n",
      "Avg Reward Episode 19: -28.603039963498034\n",
      "Avg Reward Episode 20: -27.838064619113016\n",
      "Avg Reward Episode 21: -29.035965922554013\n",
      "Avg Reward Episode 22: -27.79604304363328\n",
      "Avg Reward Episode 23: -29.549806172310955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  27%|██▋       | 27/100 [00:01<00:02, 29.54it/s, Episode Reward:=Epoch: 30 Reward: -29.76017907193722] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 24: -29.769743198644182\n",
      "Avg Reward Episode 25: -28.879933820448873\n",
      "Avg Reward Episode 26: -27.618261139362392\n",
      "Avg Reward Episode 27: -28.48046642789545\n",
      "Avg Reward Episode 28: -29.557622010351874\n",
      "Avg Reward Episode 29: -27.191142169602724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  34%|███▍      | 34/100 [00:01<00:02, 29.77it/s, Episode Reward:=Epoch: 35 Reward: -28.16894566398108] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 30: -29.76017907193722\n",
      "Avg Reward Episode 31: -28.652410733969557\n",
      "Avg Reward Episode 32: -28.744126978679667\n",
      "Avg Reward Episode 33: -26.386956879234233\n",
      "Avg Reward Episode 34: -30.015453689675994\n",
      "Avg Reward Episode 35: -28.16894566398108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  40%|████      | 40/100 [00:01<00:02, 29.34it/s, Episode Reward:=Epoch: 41 Reward: -29.003321780908077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 36: -28.147443598962603\n",
      "Avg Reward Episode 37: -28.82108676269764\n",
      "Avg Reward Episode 38: -28.38581882053384\n",
      "Avg Reward Episode 39: -29.106267088817276\n",
      "Avg Reward Episode 40: -27.88374882939069\n",
      "Avg Reward Episode 41: -29.003321780908077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  46%|████▌     | 46/100 [00:01<00:01, 28.42it/s, Episode Reward:=Epoch: 47 Reward: -29.366944382897675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 42: -28.48737593127528\n",
      "Avg Reward Episode 43: -26.160059091879642\n",
      "Avg Reward Episode 44: -27.369309665537394\n",
      "Avg Reward Episode 45: -28.282330902732436\n",
      "Avg Reward Episode 46: -28.697380926516136\n",
      "Avg Reward Episode 47: -29.366944382897675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  52%|█████▏    | 52/100 [00:01<00:01, 28.65it/s, Episode Reward:=Epoch: 54 Reward: -28.303684968512204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 48: -27.656856599820955\n",
      "Avg Reward Episode 49: -28.661682008518337\n",
      "Avg Reward Episode 50: -28.963454700094307\n",
      "Avg Reward Episode 51: -28.40714964359916\n",
      "Avg Reward Episode 52: -29.047752452346653\n",
      "Avg Reward Episode 53: -29.009977139678295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  58%|█████▊    | 58/100 [00:02<00:01, 25.67it/s, Episode Reward:=Epoch: 58 Reward: -27.621714692117838]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 54: -28.303684968512204\n",
      "Avg Reward Episode 55: -27.017169459738945\n",
      "Avg Reward Episode 56: -28.49285169542332\n",
      "Avg Reward Episode 57: -29.210331599048505\n",
      "Avg Reward Episode 58: -27.621714692117838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  64%|██████▍   | 64/100 [00:02<00:01, 27.28it/s, Episode Reward:=Epoch: 65 Reward: -29.736424409562407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 59: -28.112935373714077\n",
      "Avg Reward Episode 60: -28.800645147161138\n",
      "Avg Reward Episode 61: -29.0391051775712\n",
      "Avg Reward Episode 62: -28.071035369076913\n",
      "Avg Reward Episode 63: -29.590372351903376\n",
      "Avg Reward Episode 64: -28.74379140618233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  70%|███████   | 70/100 [00:02<00:01, 27.65it/s, Episode Reward:=Epoch: 70 Reward: -28.602637888173305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 65: -29.736424409562407\n",
      "Avg Reward Episode 66: -25.662392512227406\n",
      "Avg Reward Episode 67: -26.934082988637215\n",
      "Avg Reward Episode 68: -28.858684760326955\n",
      "Avg Reward Episode 69: -27.24842557425831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  73%|███████▎  | 73/100 [00:02<00:00, 27.53it/s, Episode Reward:=Epoch: 75 Reward: -29.489551024183406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 70: -28.602637888173305\n",
      "Avg Reward Episode 71: -29.181979863625322\n",
      "Avg Reward Episode 72: -29.221052344933028\n",
      "Avg Reward Episode 73: -25.333161636324192\n",
      "Avg Reward Episode 74: -28.949943498495166\n",
      "Avg Reward Episode 75: -29.489551024183406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  82%|████████▏ | 82/100 [00:02<00:00, 27.93it/s, Episode Reward:=Epoch: 81 Reward: -29.031400648657034]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 76: -29.288436708162426\n",
      "Avg Reward Episode 77: -26.298428938453977\n",
      "Avg Reward Episode 78: -26.48007493790066\n",
      "Avg Reward Episode 79: -28.869000606454033\n",
      "Avg Reward Episode 80: -28.153051395157178\n",
      "Avg Reward Episode 81: -29.031400648657034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  85%|████████▌ | 85/100 [00:03<00:00, 27.45it/s, Episode Reward:=Epoch: 86 Reward: -29.29743833338935] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 82: -29.347075978893876\n",
      "Avg Reward Episode 83: -27.185803290080585\n",
      "Avg Reward Episode 84: -29.514413146333872\n",
      "Avg Reward Episode 85: -27.671224899351547\n",
      "Avg Reward Episode 86: -29.29743833338935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  91%|█████████ | 91/100 [00:03<00:00, 24.97it/s, Episode Reward:=Epoch: 92 Reward: -29.184979068656936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 87: -29.810438945689672\n",
      "Avg Reward Episode 88: -29.21750161636382\n",
      "Avg Reward Episode 89: -29.215208479931313\n",
      "Avg Reward Episode 90: -27.676486893395477\n",
      "Avg Reward Episode 91: -27.912941903100762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  97%|█████████▋| 97/100 [00:03<00:00, 26.04it/s, Episode Reward:=Epoch: 97 Reward: -29.6903153152279]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 92: -29.184979068656936\n",
      "Avg Reward Episode 93: -28.879336780790098\n",
      "Avg Reward Episode 94: -28.390825723372167\n",
      "Avg Reward Episode 95: -27.704010217923972\n",
      "Avg Reward Episode 96: -29.735791041820068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward Episode 97: -29.6903153152279\n",
      "Avg Reward Episode 98: -28.112210452350478\n",
      "Avg Reward Episode 99: -28.496289726213565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "DO_RENDER = False\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "mag_coords = [np.array([0.,-1.,3.]),np.array([0.,1.,3.])]\n",
    "spawn_range = ((-0.1,0.1),(-0.1,0.1),(0,1))\n",
    "desired_range = ((0,0),(0,0),(0.1,0.9))\n",
    "# Create and wrap the environment\n",
    "env = MagneticEnv(mag_coords, DT)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "total_num_episodes = int(100)  # Total number of episodes NOTE switch back to int(5e3) once done debugging\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "\n",
    "# Reinitialize agent every seed\n",
    "agent = Policy(obs_space_dims, action_space_dims)\n",
    "reward_over_episodes = []\n",
    "\n",
    "#Tqdm progress bar object contains a list of the batch indices to train over\n",
    "progress_bar = tqdm(range(total_num_episodes), desc='Training...', leave=False, disable=False)\n",
    "\n",
    "for episode in progress_bar:\n",
    "    obs, info = wrapped_env.reset(seed=RANDOM_SEED, options=(spawn_range,desired_range))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.sample_action(obs)\n",
    "        obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "        if DO_RENDER: env.render()\n",
    "\n",
    "        done = terminated or truncated\n",
    "    \n",
    "\n",
    "    avg_reward = sum(agent.rewards)/len(agent.rewards)\n",
    "    progress_bar.set_postfix({\"Episode Reward:\": f\"Epoch: {episode} Reward: {avg_reward}\"})\n",
    "    print(f\"Avg Reward Episode {episode}: {avg_reward}\")\n",
    "    reward_over_episodes.append(avg_reward)\n",
    "    agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xs = np.arange(len(reward_over_episodes))\n",
    "# Fit a linear trendline\n",
    "coefficients = np.polyfit(xs, reward_over_episodes, 1)\n",
    "trendline = np.polyval(coefficients, xs)\n",
    "\n",
    "# Plot the original data points \n",
    "plt.plot(xs, reward_over_episodes, label='Data Points')\n",
    "\n",
    "# Plot the trendline\n",
    "plt.plot(xs, trendline, color='red', label='Trendline')\n",
    "slope = coefficients[0]\n",
    "plt.text(0.5, 25, f'Slope: {slope:.2f}', fontsize=12, color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Reward Over Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
