{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import keyboard\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from maglev_env import MagneticEnv, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE think more about these values\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 16\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        # NOTE do we want relu on this?\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         for each normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted means of the action space's normal distribution\n",
    "            action_stddevs: predicted standard deviation of the action space's normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs\n",
    "\n",
    "class Policy:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm.\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        self.action_space_dims = action_space_dims\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-4  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns action(s), conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: Action(s) to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        action_means = action_means.squeeze()\n",
    "        action_stddevs = action_stddevs.squeeze()\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample all actions action\n",
    "        actions = np.zeros(self.action_space_dims)\n",
    "        for action_dim in range(self.action_space_dims):\n",
    "            distrib = Normal(action_means[action_dim] + self.eps, action_stddevs[action_dim] + self.eps)\n",
    "            action = distrib.sample()\n",
    "            prob = distrib.log_prob(action)\n",
    "            actions[action_dim] = action.numpy()\n",
    "\n",
    "            self.probs.append(prob)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "DO_RENDER = False\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "mag_coords = [np.array([0.,-1.,3.]),np.array([0.,1.,3.])]\n",
    "spawn_range = ((-0.1,0.1),(-0.1,0.1),(0,1))\n",
    "desired_range = ((0,0),(0,0),(0.1,0.9))\n",
    "# Create and wrap the environment\n",
    "env = MagneticEnv(mag_coords, DT)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "total_num_episodes = int(5e3)  # Total number of episodes\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "\n",
    "# Reinitialize agent every seed\n",
    "agent = Policy(obs_space_dims, action_space_dims)\n",
    "reward_over_episodes = []\n",
    "\n",
    "#Tqdm progress bar object contains a list of the batch indices to train over\n",
    "progress_bar = tqdm(range(total_num_episodes), desc='Training...', leave=False, disable=False)\n",
    "\n",
    "for episode in progress_bar:\n",
    "    obs, info = wrapped_env.reset(seed=RANDOM_SEED, options=(spawn_range,desired_range))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.sample_action(obs)\n",
    "        obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "\n",
    "        if DO_RENDER: env.render()\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "    avg_reward = sum(agent.rewards)/len(agent.rewards)\n",
    "    progress_bar.set_postfix({\"Episode Reward:\": f\"Epoch: {episode} Reward: {avg_reward}\"})\n",
    "    # print(f\"Avg Reward Episode {episode}: {avg_reward}\")\n",
    "    reward_over_episodes.append(avg_reward)\n",
    "    agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xs = np.arange(len(reward_over_episodes))\n",
    "# Fit a linear trendline\n",
    "coefficients = np.polyfit(xs, reward_over_episodes, 1)\n",
    "trendline = np.polyval(coefficients, xs)\n",
    "\n",
    "# Plot the original data points \n",
    "plt.plot(xs, reward_over_episodes, label='Data Points')\n",
    "\n",
    "# Plot the trendline\n",
    "plt.plot(xs, trendline, color='red', label='Trendline')\n",
    "slope = coefficients[0]\n",
    "plt.text(0.5, 25, f'Slope: {slope:.2f}', fontsize=12, color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Reward Over Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
