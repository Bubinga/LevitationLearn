{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from maglev_env import MagneticEnv, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE think more about these values\n",
    "        hidden_space1 = 128\n",
    "        hidden_space2 = 128\n",
    "        hidden_space3 = 128\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space2, hidden_space3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space3, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        # NOTE do we want relu on this?\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space3, action_space_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         for each normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted means of the action space's normal distribution\n",
    "            action_stddevs: predicted standard deviation of the action space's normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs\n",
    "\n",
    "class Policy:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int, electromagnets, device=\"cpu\"):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm.\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        self.action_space_dims = action_space_dims\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 5e-4  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "        self.device = device\n",
    "        self.electromag_pos = electromagnets\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims).to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def augment_obs(self, obs):\n",
    "        # obs is 9x1 of XYZ (ball.position,ball.velocity,desired_position)\n",
    "        ext_data = torch.zeros((1,1 + len(self.electromag_pos))).to(self.device)\n",
    "        aug_obs = torch.cat((obs, ext_data), axis = 1)\n",
    "        # Error\n",
    "        aug_obs[0][9] = torch.linalg.norm(obs[0][6:9] - obs[0][:3])\n",
    "        for i, mag_pos in enumerate(self.electromag_pos):\n",
    "            # Distance to electromagnet\n",
    "            aug_obs[0][10+i] = torch.linalg.norm(torch.tensor(mag_pos) - obs[0][:3])\n",
    "        return aug_obs\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns action(s), conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment nx1\n",
    "        Returns:\n",
    "            action: Action(s) to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state])).float().to(self.device)\n",
    "        # state = torch.tensor(np.array([state]))\n",
    "        state = self.augment_obs(state)\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "        \n",
    "        action_means = action_means.flatten()\n",
    "        action_stddevs = action_stddevs.flatten()\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample all actions action\n",
    "        actions = np.zeros(self.action_space_dims)\n",
    "        for action_dim in range(self.action_space_dims):\n",
    "            distrib = Normal(action_means[action_dim] + self.eps, action_stddevs[action_dim] + self.eps)\n",
    "            action = distrib.sample()\n",
    "            prob = distrib.log_prob(action)\n",
    "            actions[action_dim] = action\n",
    "\n",
    "            self.probs.append(prob)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs).to(self.device)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" # current implementation has cpu being faster\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def generate_points_on_circle(radius, center, z, n):\n",
    "    h, k = center\n",
    "    angles = np.linspace(0, 2*np.pi, n, endpoint=False)  # Generate equally spaced angles\n",
    "    x = h + radius * np.cos(angles)\n",
    "    y = k + radius * np.sin(angles)\n",
    "    z_values = np.full_like(x, z)  # Create an array filled with the z value\n",
    "    points = np.column_stack((x, y, z_values))\n",
    "    return points\n",
    "\n",
    "# Example usage:\n",
    "radius = 2\n",
    "center = (0, 0)\n",
    "num_points = 8\n",
    "\n",
    "circle_points = generate_points_on_circle(radius, center, 0, num_points)\n",
    "mag_coords = list(circle_points)\n",
    "mag_coords.append(np.array([0,0,0]))\n",
    "\n",
    "# mag_coords = [np.array([0,0,0]),]\n",
    "spawn_range = ((-1.5,1.5),(-1.5,1.5),(2,3))\n",
    "desired_range = ((-1.2,1.2),(-1.2,1.2),(2,2.8))\n",
    "# Create and wrap the environment\n",
    "env = MagneticEnv(mag_coords, DT, spawn_range, desired_range)\n",
    "record_int = 10\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, record_int +1)  # Records episode-reward\n",
    "\n",
    "# Reinitialize agent every seed\n",
    "electro_positions = [e.position for e in env.electromagnets]\n",
    "\n",
    "# extra dimensions for engineered features \n",
    "# 1 for error n for electromag distance\n",
    "obs_space_dims = env.observation_space.shape[0] + 1 + len(electro_positions)\n",
    "action_space_dims = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.rc_file_defaults()\n",
    "# sns.reset_orig()\n",
    "# plt.clf()\n",
    "\n",
    "# env.fig = plt.figure()\n",
    "# env.ax = env.fig.add_subplot(111, projection=\"3d\")\n",
    "# env.fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize agent and record\n",
    "agent = Policy(obs_space_dims, action_space_dims,electro_positions, device)\n",
    "reward_over_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 36/75000 [02:18<83:12:49,  4.00s/it, Stats=Epoch: 35, Avg Reward: -847, Reward [-1007.1406]] "
     ]
    }
   ],
   "source": [
    "DO_RENDER = True\n",
    "total_num_episodes = int(75_000)  # Total number of episodes\n",
    "\n",
    "#Tqdm progress bar object contains a list of the batch indices to train over\n",
    "progress_bar = tqdm(range(total_num_episodes), desc='Training...', leave=False, disable=False)\n",
    "\n",
    "for episode in progress_bar:\n",
    "    obs, info = wrapped_env.reset(seed=RANDOM_SEED)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.sample_action(obs)\n",
    "        obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "\n",
    "        if DO_RENDER: wrapped_env.env.render()\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if episode % record_int == 0:\n",
    "        avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "        reward_over_episodes.append(avg_reward)\n",
    "\n",
    "    reward_val = wrapped_env.return_queue[-1]\n",
    "    \n",
    "    progress_bar.set_postfix({\"Stats\": f\"Epoch: {episode}, Avg Reward: {avg_reward}, Reward {reward_val}\"})\n",
    "    agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = MagneticEnv(mag_coords, DT)\n",
    "record_int = 10\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, record_int +1)  # Records episode-reward\n",
    "\n",
    "model = SAC(\"MlpPolicy\", wrapped_env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4, progress_bar=True)\n",
    "obs, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "    wrapped_env.env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_to_plot = np.array(reward_over_episodes)\n",
    "xs = np.arange(len(rewards_to_plot)) * record_int\n",
    "\n",
    "result_fig = plt.figure()\n",
    "plt.clf()\n",
    "result_ax = result_fig.add_subplot(111)\n",
    "\n",
    "coef = np.polyfit(xs,rewards_to_plot,1)\n",
    "result_ax.plot(xs, rewards_to_plot, label= \"Rewards\")\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "\n",
    "result_ax.plot(xs, poly1d_fn(xs), '--r', label= \"Learning Loss\" ) #'--k'=black dashed line, 'yo' = yellow circle marker\n",
    "result_ax.set_xlabel(\"Episodes\")\n",
    "result_ax.set_ylabel(\"Reward\")\n",
    "result_ax.set_title(\"Levitation Learn\")\n",
    "result_ax.set_ylim([-12000,7000])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
