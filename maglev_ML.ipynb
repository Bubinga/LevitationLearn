{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import keyboard\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from maglev_env import MagneticEnv, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE think more about these values\n",
    "        hidden_space1 = 32\n",
    "        hidden_space2 = 32\n",
    "        hidden_space3 = 32\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space2, hidden_space3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space3, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        # NOTE do we want relu on this?\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space3, action_space_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         for each normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted means of the action space's normal distribution\n",
    "            action_stddevs: predicted standard deviation of the action space's normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs\n",
    "\n",
    "class Policy:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int, electromagnets, device=\"cpu\"):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm.\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        self.action_space_dims = action_space_dims\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-3  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "        self.device = device\n",
    "        self.electromagnets = electromagnets\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims).to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def augment_obs(self, obs):\n",
    "        # obs is 9x1 of XYZ (ball.position,ball.velocity,desired_position)\n",
    "        ext_data = torch.zeros((1,1+3*len(self.electromagnets))).to(self.device)\n",
    "        aug_obs = torch.cat((obs, ext_data), axis = 1)\n",
    "        # Error\n",
    "        aug_obs[0][9] = torch.linalg.norm(obs[0][6:9] - obs[0][:3])\n",
    "        for i in range(len(self.electromagnets)):\n",
    "            # Distance to electromagnet\n",
    "            aug_obs[(10+3*i):(10+3*i+3)] = torch.linalg.norm(obs[0][6:9] - obs[0][:3])\n",
    "        return aug_obs\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns action(s), conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment nx1\n",
    "        Returns:\n",
    "            action: Action(s) to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state])).float().to(self.device)\n",
    "        # state = torch.tensor(np.array([state]))\n",
    "        state = self.augment_obs(state)\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "        \n",
    "        action_means = action_means.flatten()\n",
    "        action_stddevs = action_stddevs.flatten()\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample all actions action\n",
    "        actions = np.zeros(self.action_space_dims)\n",
    "        for action_dim in range(self.action_space_dims):\n",
    "            distrib = Normal(action_means[action_dim] + self.eps, action_stddevs[action_dim] + self.eps)\n",
    "            action = distrib.sample()\n",
    "            prob = distrib.log_prob(action)\n",
    "            actions[action_dim] = action\n",
    "\n",
    "            self.probs.append(prob)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs).to(self.device)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" # current implementation has cpu being faster\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "mag_coords = [np.array([0,0,4]),]\n",
    "spawn_range = ((-0.1,0.1),(-0.1,0.1),(0,1))\n",
    "desired_range = ((0,0),(0,0),(0.5,1.5))\n",
    "# Create and wrap the environment\n",
    "env = MagneticEnv(mag_coords, DT)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "# Reinitialize agent every seed\n",
    "electro_positions = [e.position for e in env.electromagnets]\n",
    "\n",
    "# 2 extra dimensions for engineered features\n",
    "obs_space_dims = env.observation_space.shape[0] + 1 + 3*len(electro_positions)\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "\n",
    "rewards_over_seeds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.fig = plt.figure()\n",
    "env.ax = env.fig.add_subplot(111, projection=\"3d\")\n",
    "env.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tqdm progress bar object contains a list of the batch indices to train over\n",
    "DO_RENDER = True\n",
    "total_num_episodes = int(5e3)  # Total number of episodes\n",
    "progress_bar = tqdm(range(total_num_episodes), desc='Training...', leave=False, disable=False)\n",
    "\n",
    "for seed in [43]:  # Fibonacci seeds\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Reinitialize agent every seed\n",
    "    agent = Policy(obs_space_dims, action_space_dims,electro_positions, device)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    for episode in progress_bar:\n",
    "        obs, info = wrapped_env.reset(seed=RANDOM_SEED, options=(spawn_range,desired_range))\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # prev_obs = obs.copy()\n",
    "            action = agent.sample_action(obs)\n",
    "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "            # if reward > 200:\n",
    "            #     pass #for breakpoints\n",
    "            if DO_RENDER: wrapped_env.env.render()\n",
    "\n",
    "            done = terminated or truncated\n",
    "            # if terminated: print(\"position reached successfully\")\n",
    "\n",
    "        reward_val = wrapped_env.return_queue[-1]\n",
    "        progress_bar.set_postfix({\"Episode Reward:\": f\"Epoch: {episode}, Reward {reward_val}\"})\n",
    "        if episode % 200 == 0:\n",
    "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "        agent.update()\n",
    "        reward_over_episodes.append(reward_val)\n",
    "    rewards_over_seeds.append(reward_over_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rewards and clip values\n",
    "# rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
    "rewards_to_plot = np.array(rewards_over_seeds)\n",
    "rewards_to_plot = rewards_to_plot.flatten()\n",
    "# rewards_to_plot = rewards_to_plot[8000:15000]\n",
    "xs = np.arange(len(rewards_to_plot))\n",
    "# rewards_to_plot = np.stack((xs,rewards_to_plot))\n",
    "rewards_to_plot = np.expand_dims(rewards_to_plot, axis=0)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
    "df1.rename(columns={\"variable\": \"Episodes\", \"value\": \"Reward\"}, inplace=True)\n",
    "\n",
    "# Plot the line plot with a trendline\n",
    "ax = sns.lineplot(x=\"Episodes\", y=\"Reward\", data=df1)\n",
    "sns.regplot(x=\"Episodes\", y=\"Reward\", data=df1, ax=ax, scatter=False, color='red')\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "ax.set_ylim(-5000,10000)\n",
    "# Add a trendline using regplot\n",
    "\n",
    "ax.set(title=\"REINFORCE for Levitation Learn (Attempt#1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc_file_defaults()\n",
    "sns.reset_orig()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
